{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"gpuClass":"standard","accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"46oOIgSUpoVM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669367392388,"user_tz":-480,"elapsed":20380,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}},"outputId":"10f902b5-f87b-470b-a911-8957bf098b04"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","metadata":{"id":"GdJtNno3rXjB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669367417350,"user_tz":-480,"elapsed":9028,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}},"outputId":"8a01895e-5842-4067-fb58-8e58d55a917d"},"source":["!pip install transformers"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.24.0-py3-none-any.whl (5.5 MB)\n","\u001b[K     |████████████████████████████████| 5.5 MB 9.2 MB/s \n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[K     |████████████████████████████████| 7.6 MB 64.0 MB/s \n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.13.0)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.0-py3-none-any.whl (182 kB)\n","\u001b[K     |████████████████████████████████| 182 kB 74.3 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.1.1)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.10.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.9.24)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.0 tokenizers-0.13.2 transformers-4.24.0\n"]}]},{"cell_type":"code","metadata":{"id":"zMedwkzaz5na","executionInfo":{"status":"ok","timestamp":1669367706941,"user_tz":-480,"elapsed":4,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"source":["import os\n","\n","os.chdir(\"drive/MyDrive/Colab Notebooks/Unet\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import pandas as pd\n","import numpy as np\n","import shutil\n","import sys\n","from PIL import Image\n","import json\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","import torchvision.transforms as transforms\n","from tqdm import tqdm\n","from transformers import get_scheduler\n","from torchvision.utils import save_image"],"metadata":{"id":"y8gqJOWFnZhQ","executionInfo":{"status":"ok","timestamp":1669367714552,"user_tz":-480,"elapsed":6860,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["# definition of all blocks\n","class block(nn.Module):\n","  def __init__(self, in_channels, out_channels):\n","    super().__init__()\n","    self.conv1 = nn.Conv2d(in_channels, out_channels, 3)\n","    self.relu  = nn.ReLU()\n","    self.conv2 = nn.Conv2d(out_channels, out_channels, 3)\n","\n","  def forward(self, x):\n","    return self.conv2(self.relu(self.conv1(x)))\n","\n","class encoder(nn.Module):\n","  def __init__(self, channels=(3,16,32,64)):\n","    super().__init__()\n","    self.blocks = nn.ModuleList([block(channels[i], channels[i+1]) for i in range(len(channels)-1)])\n","    self.pool = nn.MaxPool2d(2)\n","  def forward(self, x):\n","    features = []\n","    for b in self.blocks:\n","      x = b(x)\n","      features.append(x)\n","      x = self.pool(x)\n","    return features\n","\n","class decoder(nn.Module):\n","  def __init__(self, channels=(64, 32, 16)):\n","    super().__init__()\n","    self.channels = channels\n","    self.upconvs = nn.ModuleList([nn.ConvTranspose2d(channels[i], channels[i+1], 2, 2) for i in range(len(channels)-1)])\n","    self.blocks = nn.ModuleList([block(channels[i], channels[i+1]) for i in range(len(channels)-1)])\n","\n","  def crop(self, encoder_features, x):\n","    _, _, H, W = x.shape\n","    encoder_features = transforms.CenterCrop([H, W])(encoder_features)\n","    return encoder_features\n","  \n","  def forward(self, x, encoder_features):\n","    for i in range(len(self.channels)-1):\n","      x = self.upconvs[i](x)\n","      enc_ftrs  = self.crop(encoder_features[i], x)\n","      x = torch.cat([x, enc_ftrs], dim=1)\n","      x = self.blocks[i](x)\n","    return x"],"metadata":{"id":"U4FwRMMS-sNj","executionInfo":{"status":"ok","timestamp":1669367715275,"user_tz":-480,"elapsed":4,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["# Unet\n","class UNet(nn.Module):\n","  def __init__(self, encoder_channels=(3, 16, 32, 64), decoder_channels=(64, 32, 16), num_class=1, retain_dim=True, out_sz=(128, 128)):\n","    super().__init__()\n","    self.encoder = encoder(encoder_channels)\n","    self.decoder = decoder(decoder_channels)\n","    self.head = nn.Conv2d(decoder_channels[-1], num_class, 1)\n","    self.retain_dim = retain_dim\n","    self.out_sz = out_sz\n","\n","  def forward(self, x):\n","    encoder_features = self.encoder(x)\n","    out = self.decoder(encoder_features[::-1][0], encoder_features[::-1][1:])\n","    out = self.head(out)\n","    if self.retain_dim:\n","        out = F.interpolate(out, self.out_sz)\n","    return out"],"metadata":{"id":"6FR4-1tYDfrl","executionInfo":{"status":"ok","timestamp":1669367716865,"user_tz":-480,"elapsed":4,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["transform_images = transforms.Compose(\n","        [\n","            transforms.Resize((128, 128)),\n","            transforms.ToTensor(),\n","            transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n","        ]\n","    )\n","transform_labels = transforms.Compose(\n","        [\n","            transforms.Resize((128, 128)),\n","            transforms.ToTensor(),\n","        ]\n","    )"],"metadata":{"id":"eq1EJ0AxGMOT","executionInfo":{"status":"ok","timestamp":1669367720046,"user_tz":-480,"elapsed":298,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["class ipt_dataset(Dataset):\n","  def __init__(self, root_dir, annotation_file, transform_images=None, transform_labels=None):\n","    self.root_dir = root_dir\n","    self.annotations = pd.read_csv(annotation_file)\n","\n","    # feature extraction\n","    self.transform_images = transform_images\n","    self.transform_labels = transform_labels\n","\n","  def __len__(self):\n","    return len(self.annotations)\n","\n","  def __getitem__(self, index):\n","    img_id = self.annotations.iloc[index, 0]\n","    img = Image.open(os.path.join(self.root_dir, \"images\" , f\"{img_id}.png\")).convert(\"RGB\")\n","    label = Image.open(os.path.join(self.root_dir, \"masks\" , f\"{img_id}.png\")).convert(\"1\")\n","    img = self.transform_images(img)\n","    label = self.transform_labels(label)\n","\n","    return (img, label)"],"metadata":{"id":"6mWyCiWvz_HZ","executionInfo":{"status":"ok","timestamp":1669367721740,"user_tz":-480,"elapsed":409,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["device = (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","num_epochs = 5\n","learning_rate = 2e-5\n","batch_size = 16\n","shuffle = True\n","num_workers = 1\n","\n","dataset = ipt_dataset(\"train\",\"train.csv\", transform_images=transform_images, transform_labels=transform_labels)\n","train_loader = DataLoader(dataset=dataset, shuffle=shuffle, batch_size=batch_size,num_workers=num_workers, pin_memory=True)"],"metadata":{"id":"0piAu-RQP96p","executionInfo":{"status":"ok","timestamp":1669367725816,"user_tz":-480,"elapsed":4,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["model = UNet().to(device)\n","criterion = nn.BCEWithLogitsLoss()\n","optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n","\n","learning_rate_scheduler = get_scheduler(\n","        \"linear\",\n","        optimizer=optimizer,\n","        num_warmup_steps=0,\n","        num_training_steps=num_epochs * len(train_loader),\n","    )"],"metadata":{"id":"u_1JgSVPTQld","executionInfo":{"status":"ok","timestamp":1669367730882,"user_tz":-480,"elapsed":2791,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["# training\n","for epoch in range(num_epochs):\n","  model.train()\n","  loop = tqdm(train_loader, total = len(train_loader), leave = True)\n","  for imgs, labels in loop:\n","    imgs = imgs.to(device)\n","    labels = labels.to(device)\n","    \n","    optimizer.zero_grad()\n","    outputs = model(imgs)\n","    loss = criterion(outputs, labels)\n","    loss.backward()\n","    optimizer.step()\n","    learning_rate_scheduler.step()\n","    loop.set_description(f\"Epoch [{epoch+1}/{num_epochs}]\")\n","    loop.set_postfix(loss = loss.item())\n","  checkpoint = {\n","                'epoch': epoch,\n","                'state_dict': model.state_dict(),\n","                'optimizer': optimizer.state_dict()\n","  }\n","  torch.save(checkpoint, \"checkpoint\")"],"metadata":{"id":"fZdkmea5Qxui","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1669370094640,"user_tz":-480,"elapsed":2362059,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}},"outputId":"9aa5f216-ae2a-40dd-d250-01902d520d4b"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["Epoch [1/5]: 100%|██████████| 4000/4000 [34:06<00:00,  1.95it/s, loss=1.49]\n","Epoch [2/5]: 100%|██████████| 4000/4000 [01:17<00:00, 51.56it/s, loss=0.336]\n","Epoch [3/5]: 100%|██████████| 4000/4000 [01:16<00:00, 52.44it/s, loss=0.23]\n","Epoch [4/5]: 100%|██████████| 4000/4000 [01:17<00:00, 51.33it/s, loss=0.684]\n","Epoch [5/5]: 100%|██████████| 4000/4000 [01:19<00:00, 50.16it/s, loss=0.67]\n"]}]},{"cell_type":"code","source":["# testing\n","model.eval()\n","loop = tqdm(train_loader, total = len(train_loader), leave = True)\n","cnt = 0\n","for imgs, labels in loop:\n","  imgs = imgs.to(device)\n","  labels = labels.to(device)\n","\n","  outputs = model(imgs)\n","  outputs = nn.Sigmoid()(outputs)\n","  save_image(imgs, f'{cnt}.png')\n","  outputs = (torch.squeeze(outputs) > 0.35).type(torch.FloatTensor)\n","  save_image(outputs, f'{cnt}_p.png')\n","  save_image(labels, f'{cnt}_t.png')\n","  cnt +=1 \n","  if cnt == 7:\n","    break\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ijYYKgZMjHIg","executionInfo":{"status":"ok","timestamp":1669371178784,"user_tz":-480,"elapsed":1025,"user":{"displayName":"張嘉榮","userId":"12715841570686937545"}},"outputId":"4c118385-0a3a-4743-ced4-5b125793c1b7"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stderr","text":["  0%|          | 6/4000 [00:00<03:14, 20.54it/s]\n"]}]}]}